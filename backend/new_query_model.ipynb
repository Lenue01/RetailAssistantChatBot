{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc50c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e17d6bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "GPU device name: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"There are {torch.cuda.device_count()} GPU(s) available.\")\n",
    "    print(f\"GPU device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using CPU instead.\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5236351",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a705d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.load_local(\"faiss_index/\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12baf583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.57s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"mistral_model\"  # Point to the local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=\"ADD ID TOKEN HERE\") #add your own token ID\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\", token=\"ADD ID TOKEN HERE\")# add your own token ID here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c2f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,  # Adjust as needed have found this is best for current model\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fb73f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10074/3438874202.py:1: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=llm_pipeline)\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd62eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a retail assistant with knowlage of company process guides.  you are to follow the process guides and assist workers with any questions they may ask.   Only use informattion found in the provided documents.  You are to assume it is a instore return unless otherwise told.  Respond in 1-2 sentences.\n",
    "\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cb1fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77923243",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the return period on Major Appliances\" #Change query to test model here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a016fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents:\n",
      "Doc 1: Return period of 2 days: Major Appliances* (e.g., Full Size Refrigerators, Freezers, Dishwashers, Oven Ranges, Cooktops, Furnaces, Trash Compactors, Wall Ovens, Washing Machine & Dryer Sets,\n",
      "\n",
      "Question: What is the return period on Major Appliances\n",
      "Answer: You are a retail assistant with knowlage of company process guides.  you are to follow the process guides and assist workers with any questions they may ask.   Only use informattion found in the provided documents.  You are to assume it is a instore return unless otherwise told.  Respond in 1-2 sentences.\n",
      "\n",
      "\n",
      "Context:\n",
      "Return period of 2 days: Major Appliances* (e.g., Full Size Refrigerators, Freezers, Dishwashers, Oven Ranges, Cooktops, Furnaces, Trash Compactors, Wall Ovens, Washing Machine & Dryer Sets,\n",
      "\n",
      "Marketplace and Consumer Electronics items (most items are returnable within 30 days).\n",
      "\n",
      "Question: What is the return period on Major Appliances\n",
      "Answer: The return period for Major Appliances is 2 days according to the process guides.\n"
     ]
    }
   ],
   "source": [
    "def truncate_context(documents, max_tokens=300, tokenizer=tokenizer):\n",
    "    truncated_docs = []\n",
    "    for doc in documents:\n",
    "        tokens = tokenizer.encode(doc.page_content, add_special_tokens=False)\n",
    "        if len(tokens) > max_tokens // len(documents):  \n",
    "            tokens = tokens[:max_tokens // len(documents)]\n",
    "            doc.page_content = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        truncated_docs.append(doc)\n",
    "    return truncated_docs\n",
    "\n",
    "try:\n",
    "    \n",
    "    docs = vectorstore.as_retriever(search_kwargs={\"k\": 1}).invoke(query) #Have found that 1 - 2 is trhe best for K\n",
    "    # Truncate context\n",
    "    docs = truncate_context(docs, max_tokens=300) #reduced to prevent over filling \n",
    "    print(\"Retrieved Documents:\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Doc {i+1}: {doc.page_content}\")\n",
    "\n",
    "    \n",
    "    response = qa_chain.invoke({\"query\": query})\n",
    "    final_answer = response.get('result')\n",
    "\n",
    "    print(f\"\\nQuestion: {query}\")\n",
    "    print(f\"Answer: {final_answer}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retail_bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
