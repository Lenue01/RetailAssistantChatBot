{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa1de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7eed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw content from return_window.txt: 2 Days\n",
      "    Return period of 2 days: Major Appliances* (e.g., Full Size Refrigerators, Freezers, Dishwashers, Oven Ranges, Cooktops, Furnaces, Trash Compactors, Wall Ovens, Washing Machine & Dryer Sets, Range Hoods, Washing Machines, Dryers, Ice makers) \n",
      "\n",
      "\n",
      "\n",
      "14 Days\n",
      "    Marketplace Collectibles or Luxury Items*\n",
      "    Most Wireless Phones\n",
      "\n",
      "\n",
      "30 Days\n",
      "    AppleCare+*\n",
      "    Cooling & Heating (e.g., Air conditioners, Heaters, Air purifiers, Humidifiers, De-humidifiers, Fans, Indoor fireplaces)\n",
      "    Consumer \n",
      "Loaded 7 chunks from return_window.txt\n",
      "Raw content from restricted_returns.txt: Your item may have return restrictions and/or special instructions as follows:\n",
      "    Wireless phones, Marketplace collectible and Marketplace luxury items (most are returnable within 14 days).\n",
      "    \n",
      "    \n",
      "    Marketplace and Consumer Electronics items (most items are returnable within 30 days).\n",
      "    \n",
      "    \n",
      "    Items that are part of Consumer Electronics or Collectible bundles must all be returned together to receive a refund, downloadable software or video games are not returnable.\n",
      "    \n",
      "\n",
      "\n",
      "    Hazardou\n",
      "Loaded 12 chunks from restricted_returns.txt\n",
      "Raw content from non_return.txt: To help keep our customers and associates safe Walmart and Marketplace sellers will not replace, accept returns, or provide refunds for:\n",
      "\n",
      "    Firearms & Ammunition\n",
      "    Airsoft & Air Guns, BB Guns, Crossbows, Vertical/Archery bows are not returnable\n",
      "    Pepper Spray/Bear Spray are not returnable\n",
      "    Used or Mounted Tires, Wheels, Rims, Snow Tires are not returnable \n",
      "    Gas Powered Recreational Vehicles (e.g., Dirt Bikes, Mini-Bikes, Go-Carts, Scooters, Ride-ons, UTVs & ATVs) are not returnable\n",
      " \n",
      "Loaded 17 chunks from non_return.txt\n",
      "Total chunks loaded: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10769/512371970.py:43: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/home/lenue/Desktop/retail_bot_v1/retail_bot/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document: page_content='2 Days' metadata={'source': 'ProcessGuides/returns/return_window.txt'}\n",
      "Embedded and saved 36 chunks to 'faiss_index/'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "docs = []\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)  #decresed chunk size found this is the best for current index\n",
    "\n",
    "for root, _, files in os.walk(\"ProcessGuides\"):   #will go thru all sub folders in process guides folder\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            path = os.path.join(root, file)\n",
    "            try:\n",
    "                loader = TextLoader(path)\n",
    "                text_docs = loader.load()\n",
    "                \n",
    "                # Access the text content from the Document object\n",
    "                raw_content = text_docs[0].page_content if len(text_docs) > 0 else \"\"\n",
    "                \n",
    "                # Print the first 500 characters of the loaded content for debugging\n",
    "                print(f\"Raw content from {file}: {raw_content[:500]}\")  # Show first 500 characters\n",
    "                \n",
    "                # Split the document into chunks\n",
    "                chunks = splitter.split_documents(text_docs)\n",
    "                \n",
    "                # Print how many chunks were generated for this document\n",
    "                print(f\"Loaded {len(chunks)} chunks from {file}\")\n",
    "                \n",
    "                # Add the chunks to the docs list\n",
    "                docs.extend(chunks)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {path}: {e}\")\n",
    "\n",
    "# Check if any documents were loaded\n",
    "print(f\"Total chunks loaded: {len(docs)}\")\n",
    "if len(docs) == 0:\n",
    "    print(\"Warning: No documents were loaded. Please check the text files.\")\n",
    "\n",
    "#Load embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "#Check the document sample before creating FAISS index\n",
    "if len(docs) > 0:\n",
    "    print(f\"Sample document: {docs[0]}\")\n",
    "\n",
    "#Create and save FAISS index\n",
    "try:\n",
    "    vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "    vectorstore.save_local(\"faiss_index\")\n",
    "    print(f\"Embedded and saved {len(docs)} chunks to 'faiss_index/'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during FAISS creation: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retail_bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
